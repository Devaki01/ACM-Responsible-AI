{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYxXrXUcnvhJoDxoMP9qnm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Devaki01/ACM-Responsible-AI/blob/main/ACM_WS_GNN_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRhtnw2jr-Hl",
        "outputId": "4e6bee40-cfb1-4434-ea23-a7ce4cf5cd9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'citeseer-dataset'...\n",
            "remote: Enumerating objects: 12, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 12 (delta 1), reused 12 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (12/12), 347.81 KiB | 1014.00 KiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n",
            "Loading CiteSeer dataset...\n",
            "\n",
            "Starting Training...\n",
            "Epoch: 0000 | Loss: 1.9790 | Acc: 0.1600 | Val Acc: 0.1900\n",
            "Epoch: 0020 | Loss: 0.2169 | Acc: 0.9400 | Val Acc: 0.6533\n",
            "Epoch: 0040 | Loss: 0.0895 | Acc: 0.9800 | Val Acc: 0.6800\n",
            "Epoch: 0060 | Loss: 0.0703 | Acc: 0.9900 | Val Acc: 0.6833\n",
            "Epoch: 0080 | Loss: 0.0428 | Acc: 0.9950 | Val Acc: 0.6867\n",
            "Epoch: 0100 | Loss: 0.0501 | Acc: 0.9900 | Val Acc: 0.6867\n",
            "Epoch: 0120 | Loss: 0.0442 | Acc: 0.9850 | Val Acc: 0.6867\n",
            "Epoch: 0140 | Loss: 0.0406 | Acc: 0.9950 | Val Acc: 0.6867\n",
            "Epoch: 0160 | Loss: 0.0379 | Acc: 0.9900 | Val Acc: 0.6900\n",
            "Epoch: 0180 | Loss: 0.0499 | Acc: 0.9850 | Val Acc: 0.6967\n",
            "\n",
            "Test set results: Loss = 1.3337, Accuracy = 0.6750\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "\n",
        "# 1. Download the dataset from the provided GitHub repo\n",
        "if not os.path.exists('citeseer-dataset'):\n",
        "    !git clone https://github.com/ZPowerZ/citeseer-dataset.git\n",
        "\n",
        "# 2. Data Loading and Preprocessing Functions\n",
        "def encode_onehot(labels):\n",
        "    classes = set(labels)\n",
        "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
        "    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
        "    return labels_onehot\n",
        "\n",
        "def normalize_adj(mx):\n",
        "    \"\"\"Row-normalize sparse matrix (Laplacian normalization): D^-0.5 * A * D^-0.5\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
        "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
        "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
        "\n",
        "def load_data():\n",
        "    print(\"Loading CiteSeer dataset...\")\n",
        "    path = \"citeseer-dataset/\"\n",
        "\n",
        "    # Load content file: <paper_id> <word_attributes...> <class_label>\n",
        "    idx_features_labels = np.genfromtxt(f\"{path}citeseer.content\", dtype=np.dtype(str))\n",
        "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
        "    labels = encode_onehot(idx_features_labels[:, -1])\n",
        "\n",
        "    # Build graph\n",
        "    idx = np.array(idx_features_labels[:, 0], dtype=np.dtype(str))\n",
        "    idx_map = {j: i for i, j in enumerate(idx)}\n",
        "\n",
        "    # Load cites file: <cited_id> <citing_id>\n",
        "    edges_unordered = np.genfromtxt(f\"{path}citeseer.cites\", dtype=np.dtype(str))\n",
        "\n",
        "    # Filter edges where both nodes are in the content file\n",
        "    edges = []\n",
        "    for edge in edges_unordered:\n",
        "        if edge[0] in idx_map and edge[1] in idx_map:\n",
        "            edges.append([idx_map[edge[0]], idx_map[edge[1]]])\n",
        "    edges = np.array(edges)\n",
        "\n",
        "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
        "                        shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)\n",
        "\n",
        "    # Build symmetric adjacency matrix\n",
        "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "\n",
        "    # Add self-loops and normalize\n",
        "    adj = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
        "\n",
        "    # Convert to Tensors\n",
        "    features = torch.FloatTensor(np.array(features.todense()))\n",
        "    labels = torch.LongTensor(np.where(labels)[1])\n",
        "    adj = torch.FloatTensor(np.array(adj.todense()))\n",
        "\n",
        "    # Create random masks for Train/Val/Test\n",
        "    idx_rand = torch.randperm(len(labels))\n",
        "    train_mask = idx_rand[:200]\n",
        "    val_mask = idx_rand[200:500]\n",
        "    test_mask = idx_rand[500:1500]\n",
        "\n",
        "    return adj, features, labels, train_mask, val_mask, test_mask\n",
        "\n",
        "# 3. Manual GCN Layer Implementation\n",
        "class GraphConvolution(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple GCN layer: Z = A_norm * X * W + b\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
        "        self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / np.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        # 1. Feature transformation (X * W)\n",
        "        support = torch.mm(input, self.weight)\n",
        "        # 2. Message aggregation (A_norm * support)\n",
        "        output = torch.mm(adj, support)\n",
        "        return output + self.bias\n",
        "\n",
        "# 4. GCN Model\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "        super(GCN, self).__init__()\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc2 = GraphConvolution(nhid, nclass)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        x = F.relu(self.gc1(x, adj))\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = self.gc2(x, adj)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# 5. Training Setup\n",
        "adj, features, labels, train_idx, val_idx, test_idx = load_data()\n",
        "\n",
        "model = GCN(nfeat=features.shape[1],\n",
        "            nhid=16,\n",
        "            nclass=labels.max().item() + 1,\n",
        "            dropout=0.5)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "\n",
        "def accuracy(output, labels):\n",
        "    preds = output.max(1)[1].type_as(labels)\n",
        "    correct = preds.eq(labels).double()\n",
        "    correct = correct.sum()\n",
        "    return correct / len(labels)\n",
        "\n",
        "# 6. Training Loop\n",
        "print(\"\\nStarting Training...\")\n",
        "for epoch in range(200):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output = model(features, adj)\n",
        "    loss_train = F.nll_loss(output[train_idx], labels[train_idx])\n",
        "    acc_train = accuracy(output[train_idx], labels[train_idx])\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        model.eval()\n",
        "        output = model(features, adj)\n",
        "        loss_val = F.nll_loss(output[val_idx], labels[val_idx])\n",
        "        acc_val = accuracy(output[val_idx], labels[val_idx])\n",
        "        print(f'Epoch: {epoch:04d} | Loss: {loss_train.item():.4f} | Acc: {acc_train.item():.4f} | Val Acc: {acc_val.item():.4f}')\n",
        "\n",
        "# 7. Testing\n",
        "model.eval()\n",
        "output = model(features, adj)\n",
        "loss_test = F.nll_loss(output[test_idx], labels[test_idx])\n",
        "acc_test = accuracy(output[test_idx], labels[test_idx])\n",
        "print(f\"\\nTest set results: Loss = {loss_test.item():.4f}, Accuracy = {acc_test.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "\n",
        "# 1. Setup and Data Loading (Consistent with previous step)\n",
        "if not os.path.exists('citeseer-dataset'):\n",
        "    !git clone https://github.com/ZPowerZ/citeseer-dataset.git\n",
        "\n",
        "def encode_onehot(labels):\n",
        "    classes = set(labels)\n",
        "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
        "    return np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
        "\n",
        "def normalize_adj(mx):\n",
        "    \"\"\"Row-normalize sparse matrix: D^-0.5 * A * D^-0.5\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
        "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
        "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
        "\n",
        "def get_processed_data(use_self_loops=True):\n",
        "    path = \"citeseer-dataset/\"\n",
        "    idx_features_labels = np.genfromtxt(f\"{path}citeseer.content\", dtype=np.dtype(str))\n",
        "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
        "    labels = encode_onehot(idx_features_labels[:, -1])\n",
        "    idx = np.array(idx_features_labels[:, 0], dtype=np.dtype(str))\n",
        "    idx_map = {j: i for i, j in enumerate(idx)}\n",
        "    edges_unordered = np.genfromtxt(f\"{path}citeseer.cites\", dtype=np.dtype(str))\n",
        "\n",
        "    edges = np.array([[idx_map[e[0]], idx_map[e[1]]] for e in edges_unordered if e[0] in idx_map and e[1] in idx_map])\n",
        "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
        "                        shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)\n",
        "\n",
        "    # Symmetric adjacency\n",
        "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "\n",
        "    # TOGGLE SELF-LOOPS HERE\n",
        "    if use_self_loops:\n",
        "        adj = adj + sp.eye(adj.shape[0])\n",
        "\n",
        "    adj = normalize_adj(adj)\n",
        "\n",
        "    # Convert to tensors\n",
        "    features_t = torch.FloatTensor(np.array(features.todense()))\n",
        "    labels_t = torch.LongTensor(np.where(labels)[1])\n",
        "    adj_t = torch.FloatTensor(np.array(adj.todense()))\n",
        "\n",
        "    return adj_t, features_t, labels_t\n",
        "\n",
        "# 2. GCN Architecture\n",
        "class GraphConvolution(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
        "        self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "        nn.init.zeros_(self.bias)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        return torch.mm(adj, torch.mm(x, self.weight)) + self.bias\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass):\n",
        "        super(GCN, self).__init__()\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc2 = GraphConvolution(nhid, nclass)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        x = F.relu(self.gc1(x, adj))\n",
        "        x = F.dropout(x, 0.5, training=self.training)\n",
        "        return F.log_softmax(self.gc2(x, adj), dim=1)\n",
        "\n",
        "# 3. Training Function\n",
        "def train_model(adj, features, labels, name):\n",
        "    print(f\"\\n--- Training GCN ({name}) ---\")\n",
        "    idx_rand = torch.randperm(len(labels))\n",
        "    train_idx, test_idx = idx_rand[:200], idx_rand[500:1500]\n",
        "\n",
        "    model = GCN(features.shape[1], 16, labels.max().item() + 1)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "\n",
        "    best_acc = 0\n",
        "    for epoch in range(100):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        output = model(features, adj)\n",
        "        loss = F.nll_loss(output[train_idx], labels[train_idx])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 20 == 0:\n",
        "            model.eval()\n",
        "            preds = output[test_idx].max(1)[1]\n",
        "            acc = preds.eq(labels[test_idx]).double().mean()\n",
        "            best_acc = max(best_acc, acc.item())\n",
        "            print(f\"Epoch {epoch:03d} | Loss: {loss.item():.4f} | Test Acc: {acc.item():.4f}\")\n",
        "\n",
        "    return best_acc\n",
        "\n",
        "# 4. Run Comparison\n",
        "# Run with Self-Loops\n",
        "adj_with, feat, labels = get_processed_data(use_self_loops=True)\n",
        "acc_with = train_model(adj_with, feat, labels, \"With Self-Loops\")\n",
        "\n",
        "# Run without Self-Loops\n",
        "adj_without, feat, labels = get_processed_data(use_self_loops=False)\n",
        "acc_without = train_model(adj_without, feat, labels, \"Without Self-Loops\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(f\"FINAL COMPARISON (Accuracy)\")\n",
        "print(f\"GCN with Self-Loops:    {acc_with:.4f}\")\n",
        "print(f\"GCN without Self-Loops: {acc_without:.4f}\")\n",
        "print(\"=\"*30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HVaZwagv30G",
        "outputId": "e895b211-74c6-4202-a6d7-054207bb047b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training GCN (With Self-Loops) ---\n",
            "Epoch 000 | Loss: 1.8062 | Test Acc: 0.1260\n",
            "Epoch 020 | Loss: 0.1342 | Test Acc: 0.6180\n",
            "Epoch 040 | Loss: 0.0623 | Test Acc: 0.6160\n",
            "Epoch 060 | Loss: 0.0542 | Test Acc: 0.6200\n",
            "Epoch 080 | Loss: 0.0768 | Test Acc: 0.6220\n",
            "\n",
            "--- Training GCN (Without Self-Loops) ---\n",
            "Epoch 000 | Loss: 1.7992 | Test Acc: 0.1930\n",
            "Epoch 020 | Loss: 0.2187 | Test Acc: 0.5900\n",
            "Epoch 040 | Loss: 0.1697 | Test Acc: 0.5710\n",
            "Epoch 060 | Loss: 0.1016 | Test Acc: 0.5980\n",
            "Epoch 080 | Loss: 0.1158 | Test Acc: 0.6050\n",
            "\n",
            "==============================\n",
            "FINAL COMPARISON (Accuracy)\n",
            "GCN with Self-Loops:    0.6220\n",
            "GCN without Self-Loops: 0.6050\n",
            "==============================\n"
          ]
        }
      ]
    }
  ]
}